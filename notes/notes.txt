categories: cs,stat,math,econ,eess,q-bio,q-fin, rest is physics
"{category_name}+."
DATA SELECTION

get the ones that contain category cs in it
?maybe find some other threshold to not get all the papers, but it is fine, it is not that big tbf(journal_ref,scholar check using doi,get only if it has version 2)

!no further filtering leads to 32million lines(1.5gb), many of them have multiple fields like it is about physics but cs is also in the categories
!data is not ordered, at least not for dates 

# x = [5,2,1,10]

        # print(argsort(x))
        # [2, 1, 0, 3]
        # exclude end'th index
        # list_name[start : end : step]
        # a = [1, 2, 3, 4, 5, 6, 7, 8, 9]
        # # Get every second element from the list
        # # starting from the beginning
        # b = a[::2]
        # [1, 3, 5, 7, 9]
        # index -2 , means second to last element
        # you can use negative steps as well
        # test_array = [0, 2, 5, 10, 7, 9, 20, -1, 23]
        # np_array = np.asarray(a=test_array)
        # indices = np_array.argsort()[-5:][::-1]
        # print(indices)
        # ordered = np_array[indices]
        # print(ordered)

DATA PROCESSING

EDA

sample before EDA(Full data length is 728065 meaning 728000 articles as javascript objects I think)(taking 9000 as sample For a 95% confidence level with a 1% margin of error)
while EDA, may use already listed categories since I've got that list, instead of tracking sample's categories
TODO use indexed dict instead of list

TODO make heatmap look better
✅ Keyword Trends Over Time
Find most tfidf words per year and compare years         
✅ Check for Duplicate Papers
Some papers may have multiple versions in versions[].
Decide whether to keep only the latest version (by sorting update_date).
✅ Find Missing Values
Count missing values in abstract, journal-ref, doi, etc.
If many papers are missing DOIs, consider ignoring that field.

{
    year1: {
        category1: [paper_id1, paper_id2, ...],
        category2: [paper_id3, paper_id4, ...],
        ...
    },
    year2: {
        category1: [paper_id5, paper_id6, ...],
        category2: [paper_id7, paper_id8, ...],
        ...
    },
    ...
}

{year: {category: [abstracts]}}

LDA ONCESI TFIDF FORMATI:
'1998': {'cs.AI':            Word  TF-IDF Score
0   consequence      0.293831
1      formulae      0.189768
2         logic      0.189768
3  preferential      0.189768
4     diagnosis      0.169152
5           add      0.142326
6       premise      0.142326
7       compute      0.135322
8     structure      0.135322
9     algorithm      0.101491, 'cs.CC':                 Word  TF-IDF Score
0              proof      0.417029
1             method      0.417029
2  incompressibility      0.417029
3          technique      0.208514
4       successfully      0.208514
5             simple      0.208514
6           powerful      0.208514
7              power      0.208514
8                new      0.208514
9            exhibit      0.208514, 'cs.CL':              Word  TF-IDF Score
0      constraint      0.235661
1           model      0.176746
2       syntactic      0.161073
3       structure      0.161073
4        approach      0.156524
5             use      0.117831
6          enable      0.117831
7  disambiguation      0.117831
8       technique      0.099221
9         parsing      0.099221, 'cs.DM':                 Word  TF-IDF Score
0              proof      0.417029
1             method      0.417029
2  incompressibility      0.417029
3          technique      0.208514
4       successfully      0.208514
5             simple      0.208514
6           powerful      0.208514
7              power      0.208514
8                new      0.208514
9            exhibit      0.208514, 'cs.PL':           Word  TF-IDF Score
0      program      0.373002
1   reflection      0.373002
2  programming      0.279751
3      running      0.186501
4    integrate      0.186501
5   linguistic      0.186501
6     generate      0.186501
7        allow      0.186501
8    advantage      0.186501
9    technique      0.186501}, '1996': {'cs.CL':            Word  TF-IDF Score
0      semantic      0.143649
1          clue      0.143649
2    regularity      0.143649
3       grammar      0.139977
4     filtering      0.139977
5         logic      0.139977
6         magic      0.139977
7  optimization      0.139977
8   compilation      0.139977
9         allow      0.133664}, '1997': {'cs.CL':             Word  TF-IDF Score
0          model      0.283067
1         finite      0.170286
2          state      0.170286
3          parse      0.142159
4  approximation      0.113524
5        context      0.113524
6     processing      0.113524
7         speech      0.113524
8          paper      0.106619
9          train      0.106619}}

TODO self.category_list icin global variable olusturma yoluna bak

?maybe don't do lda, just use metadata cs categories. if lda, look at title,abstract
?maybe do both and use lda to discover subcategories

ben her yilin,her kategorisinin, alt kategorisini bulma amacindayim. Yani suan elimdeki her df icin ayri ayri, ama kategori ve yil information'larini da kaybetmemem lazim
bir de elimde 10 row'lu df'ler var, lda bununla napacak aq

fucks sake man, feeding lda with tfidf is so dumb
I guess I can compare top_tfidf with raw_fed_lda with tfidf_fed_lda

Future Impact Prediction: Predict emerging categories or hot topics based on historical trends and document characteristics.

json format before react:
{
  "1994": {
    "cs.CL": [
      {
        "topic_idx": 0,
        "top_words": [
          "improve",
          "obtain",
          "discuss",
          "different",
          "probability",
          "similarity",
          "context",
          "report",
          "paper",
          "parse"
        ]
      }
    ]
  },
  "1995": {
    "cs.CL": [
      {
        "topic_idx": 0,
        "top_words": [
          "accuracy",
          "speech",
          "set",
          "sentence",
          "semantic",
          "search",
          "context",
          "highly",
          "repeat",
          "different"
        ]
      }
    ]
  },
  "1996": {
    "cs.CL": [
      {
        "topic_idx": 0,
        "top_words": [
          "discuss",
          "analysis",
          "semantic",
          "set",
          "processing",
          "filter",
          "result",
          "paper",
          "syntactic",
          "improve"
        ]
      }
    ]
  },
  "1997": {
    "cs.CL": [
      {
        "topic_idx": 0,
        "top_words": [
          "yield",
          "eliminate",
          "error",
          "essentially",
          "evaluate",
          "experiment",
          "experimental",
          "explore",
          "extensive",
          "fast"
        ]
      }
    ]
  },
  "1998": {
    "cs.AI": [
      {
        "topic_idx": 0,
        "top_words": [
          "result",
          "introduce",
          "syntactic",
          "paper",
          "sentence",
          "complexity",
          "new",
          "discuss",
          "important",
          "improve"
        ]
      }
    ],
    "cs.CC": [
      {
        "topic_idx": 0,
        "top_words": [
          "method",
          "new",
          "yield",
          "explore",
          "free",
          "finite",
          "filter",
          "fast",
          "extensive",
          "experiment"
        ]
      }
    ],
    "cs.CL": [
      {
        "topic_idx": 0,
        "top_words": [
          "accuracy",
          "different",
          "report",
          "syntactic",
          "tag",
          "context",
          "experiment",
          "application",
          "complex",
          "recognition"
        ]
      }
    ],
    "cs.DM": [
      {
        "topic_idx": 0,
        "top_words": [
          "method",
          "new",
          "yield",
          "explore",
          "free",
          "finite",
          "filter",
          "fast",
          "extensive",
          "experiment"
        ]
      }
    ],
    "cs.PL": [
      {
        "topic_idx": 0,
        "top_words": [
          "particular",
          "new",
          "evaluate",
          "attain",
          "advantage",
          "language",
          "improve",
          "generative",
          "error",
          "essentially"
        ]
      }
    ]
  }
}


BASVURUSU SONRASI YAPILACAKLAR:(gercekten hangilerine bakarlar ona gore karar ver)
-(en son hizlica)readme ekle 
-(Onemli 2)lda-ldatfidf-tfidf karsilastirmasi koy
-(Onemli 1)eda grafiklerini guzellestir
-kodu refactor
-commit'leri duzelt 
-(Onemli)frontend'i duzelt
+(Onemli)kategori isimleri anlasilabilir yapilacak
-gereksiz dosyalari cikar, notes falan gibi + dosya duzenini duzelt
-fix the folder structure

So now main aspects of the project : Python data processing, React deployment on Vercel. For the python part, I've used an arxiv.org dataset and got a sample of  9000  For a 95% confidence level with a 1% margin of error  to first classify research for each year for each category, then I did eda to see : category distribution, year published distribution. Applied prerocessing(lowering,stopwordremoval,lemmatization) to all abstracts and continued eda with  abstract length  distribution to find average, then I applied tfidf to individual abstracts to find top idf words for each year for each category to analyze category trends over years. Then as a final point I wanted to test, I applied LDA to all abstracts per year per category, TFIDF to all abstracts per year per category, and LDA fed with TFIDF to all abstracts per year per category to discover subcategorizes of abstracts. After all of this deployed using React with visualization.

CATEGORY DISTRIBUTION

YEAR DISTRIBUTION
{"Years": [2021, 2022, 2023, 2024, 2018, 2016, 2015, 2025, 2013, 2020, 2012, 2014, 2010, 2019, 2008, 2011, 2017, 2005, 2004, 2000, 2007, 2006, 2009, 2001, 2003, 2002, 1994, 1995, 1999, 1998, 1996, 1997], "Year Counts": [977, 1088, 1238, 1483, 516, 296, 216, 237, 196, 882, 152, 202, 107, 654, 46, 119, 410, 13, 18, 4, 37, 23, 45, 5, 11, 5, 3, 4, 3, 6, 2, 2]}

abstract lengths
 {
  "Word Counts": [
    236,
    181,
    195,
    155,
    164,
    198,
    125,
    100]}

tfidf heatmap


{
"1996": [
    {
      "Word": "regularities",
      "TF-IDF Score": 0.14671288247945657
    },
    {
      "Word": "semantic",
      "TF-IDF Score": 0.14671288247945657
    },
    {
      "Word": "magic",
      "TF-IDF Score": 0.13865840046152522
    },
    {
      "Word": "compilation",
      "TF-IDF Score": 0.13865840046152522
    },
    {
      "Word": "optimizations",
      "TF-IDF Score": 0.13865840046152522
    },
    {
      "Word": "filtering",
      "TF-IDF Score": 0.13865840046152522
    },
    {
      "Word": "allows",
      "TF-IDF Score": 0.13865840046152522
    },
    {
      "Word": "logic",
      "TF-IDF Score": 0.13865840046152522
    },
    {
      "Word": "processing",
      "TF-IDF Score": 0.11891991312070155
    },
    {
      "Word": "clues",
      "TF-IDF Score": 0.09780858831963772
    }
  ],
  "1997": [
    {
      "Word": "models",
      "TF-IDF Score": 0.18148088816046706
    },
    {
      "Word": "state",
      "TF-IDF Score": 0.168683210205168
    },
    {
      "Word": "finite",
      "TF-IDF Score": 0.168683210205168
    },
    {
      "Word": "model",
      "TF-IDF Score": 0.14260958262790452
    },
    {
      "Word": "1996",
      "TF-IDF Score": 0.11610741722762549
    },
    {
      "Word": "speech",
      "TF-IDF Score": 0.112455473470112
    },
    {
      "Word": "context",
      "TF-IDF Score": 0.112455473470112
    },
    {
      "Word": "processing",
      "TF-IDF Score": 0.112455473470112
    },
    {
      "Word": "paper",
      "TF-IDF Score": 0.10695718697092839
    },
    {
      "Word": "trained",
      "TF-IDF Score": 0.10695718697092839
    }
  ]
}


json in the form of {Years:[2000,2000,2001],Words:[word_1,word_2,word_3],TF-IDF Scores:[0.2,0.5,0.1]}